# -*- coding: utf-8 -*-
"""עבודת גמר מדמ"ח.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13WZcFam66C5dbvxjcrb0gzv6yc4Kd1Vt
"""

#all of the imports (I didn't even used most in the end, but just in case)
import os
import time
from PIL import Image
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets
from torchvision import io
from torchvision import models
from torchvision import ops
from torchvision import transforms
from torchvision import utils
from torchvision.utils import save_image
from torch.autograd import Variable
from os.path import exists

"""# **Super Resolution**"""

#the core imports of the Fastai library
import fastai
from fastai.vision import *
from fastai.callbacks import *
from fastai.utils.mem import *

from torchvision.models import vgg16_bn

#the path of my DataSet taken from ImageNet
path = untar_data(URLs.PETS)

#the path of the high resolution images
path_hr = path/'images'

#the path of the low resolution images
path_lr = path/'small-96'

#the path of the medium resolution images
path_mr = path/'small-256'

#stands for 'image list'. uses ImageList.from_folder(PATH) which gets the the list
#of files in PATH that have an image suffix
il = ImageList.from_folder(path_hr)

#the basic augmentation function I used to "crapify" the images and resize them into
#smaller images for the training process
def resize_one(fn, i, path, size):
  #destination path
  dest = path/fn.relative_to(path_hr)
  dest.parent.mkdir(parents=True, exist_ok=True)

  #the image that I want to resize
  img = PIL.Image.open(fn)

  #target size
  targ_sz = resize_to(img, size, use_min=True)

  #the iage after the resize
  img = img.resize(targ_sz, resample=PIL.Image.BILINEAR).convert('RGB')

  #saves the image
  img.save(dest, quality=60)

#create smaller image sets the first time this nb is run, using the function "resize_one()"

#the sets of image which I will resize for training
sets = [(path_lr, 96), (path_mr, 256)]
for p,size in sets:
  if not p.exists(): 
    print(f"resizing to {size} into {p}")
    parallel(partial(resize_one, path=p, size=size), il.items)

#stands for batch size and, well, size (of image)
bs,size=32,128

#stands for architecture. This is the pre-traind ResNet34 from ImageNet
arch = models.resnet34

#an ItemList suitable for image to image tasks
src = ImageImageList.from_folder(path_lr).split_by_rand_pct(0.1, seed=42)

#a function that collects the data from ImageNet via its path, and the variables I've created beforehand
#the function creates a DataBunch object
def get_data(bs,size):
  #the databunch itself (of type ImageDataBunch)
  data = (src.label_from_func(lambda x: path_hr/x.name)
          .transform(get_transforms(max_zoom=2.), size=size, tfm_y=True)
          .databunch(bs=bs).normalize(imagenet_stats, do_y=True))

  data.c = 3
  return data

#the collection of all the data after collecting and sorting it
data = get_data(bs,size)

#an example to check if everyting is fine with the data
data.show_batch(ds_type=DatasetType.Valid, rows=2, figsize=(9,9))

"""# **Feature Loss**"""

#creates a gram matrix for the loss function further on
def gram_matrix(x):
  n,c,h,w = x.size()
  x = x.view(n, c, -1)
  return (x @ x.transpose(1,2))/(c*h*w)

#the base_loss will be used to compare the pixels and the features
#I'm using an L1 loss function
base_loss = F.l1_loss

#I only want the convolutional part of the VGG model
#the VGG.features attribute gives me exactly that
vgg_m = vgg16_bn(True).features.cuda().eval()
#I'm turning off the "requires_grad" because I don't want to update the weights of this model, I just want it for the loss
requires_grad(vgg_m, False)

#I will enumerate through all the children of the VGG model to find all of the max pooling layers, because
#that's where the grid size changes
#the layer i-1 is the layer before the grid size changes
#so blocks is the list of layer numbers just before the max pooling layers
blocks = [i-1 for i,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)]
blocks, [vgg_m[i] for i in blocks]

#the class FeatureLoss is my loss function for the model. It creates a loss function made from a combination
#of the VGG16 model pre-trained on ImageNet, gram matrix and L1 loss function
#note that this is perceptual loss function
class FeatureLoss(nn.Module):
  def __init__(self, m_feat, layer_ids, layer_wgts):
    super().__init__()
    
    #m_feat is the pre-traind model (VGG16) which contains the features which I want to generate for the feature loss
    self.m_feat = m_feat

    #loss_features are all of the layers from the pre-traind model from which I want the features to create the losses
    self.loss_features = [self.m_feat[i] for i in layer_ids]

    #my hooked outputs (because that's how to grab intermediate layers with pytorch)
    self.hooks = hook_outputs(self.loss_features, detach=False)
    self.wgts = layer_wgts
    self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))
          ] + [f'gram_{i}' for i in range(len(layer_ids))]

  #make_features finds the features of a given image (x) and passes it through the VGG model which contain only
  #the convolutional part of the pre-trained VGG model
  def make_features(self, x, clone=False):
    #passing the given input (x) through the VGG model
    self.m_feat(x)

    #a copy of all the stored activations 
    return [(o.clone() if clone else o) for o in self.hooks.stored]
  
  #the forward function of the perceptual loss, where we calculates and get the features of the target (y)
  #in order to create the features of the output (ŷ)
  def forward(self, input, target):
    #in out_feat i'm passing the target (y) through the VGG's stored activations and grab a copy of them,
    #thus, getting the features of the target
    out_feat = self.make_features(target, clone=True)

    #in in_feat i'm doing almost the same thing, i.e, getting the features of the input, so that the
    #output of the generator (ŷ)
    in_feat = self.make_features(input)

    #feat_losses calculates the L1 loss between the pixels
    self.feat_losses = [base_loss(input,target)]
    self.feat_losses += [base_loss(f_in, f_out)*w
                          for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]

    #calculates the L1 loss of the features of all those layers. Basically I'm going through every one
    #of these end of each block and grabbing the activations and getting the L1 loss on each one
    self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3
                          for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]

    #.metrics prints out all of the seperate layer loss amounts
    self.metrics = dict(zip(self.metric_names, self.feat_losses))
    return sum(self.feat_losses)
  
  def __del__(self): self.hooks.remove()

#this is the feature loss i'll be using in the model. Created using the
#FeatureLoss class presented before
feat_loss = FeatureLoss(vgg_m, blocks[2:5], [5,15,2])

"""# **Train**"""

#wd is weight decay
wd = 1e-3

#learn is the U-Net shaped model containing the pre-trained ResNet34
learn = unet_learner(data, arch, wd=wd, loss_func=feat_loss, callback_fns=LossMetrics,
                     blur=True, norm_type=NormType.Weight)
gc.collect();

#shows all layers in the model (learn)
learn.summary()

#.lr_find explores learning rate from start_lr (default 1e-07) to end_lr (default 10)
#over num_it iterations (default 100)
learn.lr_find()

#shows a graph of the loss versus the learning rate
learn.recorder.plot()

#check the path of the model (learn)
learn.path

#the location of the current working directory
print(os.getcwd())

#changes the path of the model if it is not saved on google drive

#IMPORTANT -----> must run so the model can be saved and/or loaded
if 'google.colab' in str(get_ipython()):
  #the google drive path
  temp_path = Path('/content/drive/MyDrive/Deep_Learning/models/')

else:
  #the current working directory
  temp_path = Path(os.gercwd())

learn.path = temp_path

#the learning rate I chose by searching the steepest loss in the learn.recorder.plot()
lr = 1e-3

#fits one cycle, saves and show the results (because I'm using a pre-trained model with frozen layers)
#I pass a slice object because in Fastai, it will give every layer in the model an lr from 0 to the
#number specified in equal spacing or if given 2 numbers inside the slice, it will give every layer
#an lr from the first number to the second number also in equal spacing
def do_fit(save_name, lrs=slice(lr), pct_start=0.9):
  #uses 1cycle policy with an optimum learning rate
  learn.fit_one_cycle(10, lrs, pct_start=pct_start)

  #saves the weights of the current training cycle in a .pth file with the given name
  learn.save(save_name)

  #.show_results() shows some predictions
  learn.show_results(rows=1, imgsize=5)

#first training cycle, called '1a' and passed a slice of 10 times lr
do_fit('1a', slice(lr*10))

#unfreezes the weights of the arch (the pre-trained ResNet)
learn.unfreeze()

#second training cycle, called '1b'. here I pass 2 numbers in the slice, that means it will give
#every layer an lr from 1e-5 to lr (1e-3) in equal spacing
do_fit('1b', slice(1e-5,lr))

#this is the progressive resizing;I doubled the size, to let the model train on better resolution
#images, so in order I won't ran out of memory, I halved the batch size
data = get_data(12,size*2)

#updating the model's (learn) data to the new databunch
learn.data = data

#freezes the weights of the arch again
learn.freeze()
gc.collect()

#the third training cycle called '2a'. Here I use the default learning rate (slice(1e-03))
do_fit('2a')

#unfreezes the weights in the pre-trained arch
learn.unfreeze()

#the fourth training cycle called '2b'
do_fit('2b', slice(1e-6,1e-4), pct_start=0.3)

#export the full model

#NOTE --> the .export() is not very effective and thus, i'm not loading the model using it
learn.export('weights.pkl')

"""# **Test**"""

#create an initial learner that is of None type
learn = None
gc.collect();

#the size of the output image if the memory is less than 8GB (820, 820) --> changed to 1024
256/320*1024

#the size of the output image if the memory is greater than 8GB (1280, 1280)
256/320*1600

#the free memory
free = gpu_mem_get_free_no_cache()

# the max size of the test image depends on the available GPU RAM 
if free > 8000:
  size=(1280, 1280) # >  8GB RAM --> size=(1280, 1600)

else:
  size=(1024, 1024) # <= 8GB RAM --> size=(820, 1024)
  
print(f"using size={size}, have {free}MB of GPU RAM free")

#the initialization of the model (learn) for the sake of validatin/testing
learn = unet_learner(data, arch, loss_func=F.l1_loss, blur=True, norm_type=NormType.Weight)

#instead of having a low-resolution data for the initialization of the model, I want to see what happens
#when initialized with the medium-resolution images. This part is not essential for the sake of the predictions
data_mr = (ImageImageList.from_folder(path_mr).split_by_rand_pct(0.1, seed=42)
          .label_from_func(lambda x: path_hr/x.name)
          .transform(get_transforms(), size=size, tfm_y=True)
          .databunch(bs=1).normalize(imagenet_stats, do_y=True))
data_mr.c = 3

#again, changing the path of the model (learn) to a google drive path

#IMPORTANT -----> must run so the model can be saved and/or loaded
if 'google.colab' in str(get_ipython()):
  #the google drive path
  temp_path = Path('/content/drive/MyDrive/Deep_Learning/models/')

else:
  #the current working directory
  temp_path = Path(os.gercwd())

learn.path = temp_path

#loading the latest training cycle weights to the model
learn.load('2b');

#updating the model's data
learn.data = data_mr

#the path of a certain image from the dataset if I want to check from there
fn = data_mr.valid_ds.x.items[0]; fn

#the input image (y). This image was resized from the original size of (1512, 1512) to a size of (450, 450)
img = open_image('/content/drive/MyDrive/Deep_Learning/my_data/img-billy2-450x450jpg-450x450'); img.shape

#the prediction

#NOTE --> the only important variable here is the img_hr which is the tensor of the prediction image (ŷ)
p,img_hr,b = learn.predict(img)

#shows the input (x) image (of size (450, 450))
show_image(img, figsize=(18,15), interpolation='nearest');

#shows the output (ŷ) image (size of (1280, 1280))
Image(img_hr).show(figsize=(18,15))

#checking the shape of the output image
print(img_hr.shape)

#the original image of size (1512, 1512), taken from my phone
img_origin = open_image('/content/drive/MyDrive/Deep_Learning/my_data/billy2.jpeg'); img_origin.shape

#shows the original image of size (1512, 1512)
show_image(img_origin, figsize=(18,15), interpolation='nearest');

from google.colab import drive
drive.mount('/content/drive')

from torchvision import transforms

data_mr = (ImageImageList.from_folder(path_mr).split_by_rand_pct(0.1, seed=42)
          .label_from_func(lambda x: path_hr/x.name)
          .transform(get_transforms(), size=size, tfm_y=True)
          .databunch(bs=10).normalize(imagenet_stats, do_y=True))
data_mr.c = 3

path_new = Path('/content/drive/MyDrive/Deep_Learning/my_data')

data_hr = (ImageImageList.from_folder(path_hr).split_by_rand_pct(0.1, seed=42)
          .label_from_func(lambda x: path_hr/x.name)
          .transform(get_transforms(), size=size, tfm_y=True)
          .databunch(bs=1).normalize(imagenet_stats, do_y=True))
data_hr.c = 3

learn.data = data_hr

print(size)

learn.show_results(rows=1, imgsize=5)

fn = data_hr.valid_ds.x.items[1]; fn

img = open_image(fn); img.shape

p,img_hr,b = learn.predict(img)

tensor = torch.load('/content/drive/MyDrive/Deep_Learning/models/tensor')

img_hr_tensor = transforms.ToPILImage('RGB')(tensor)

img_hr_tensor = PIL.Image.fromarray(image2np(tensor*255.0).astype(np.uint8))

img_hr_pil = transforms.ToPILImage('RGB')(img_hr)

show_image(img, figsize=(18,15), interpolation='nearest'); img.shape

Image(img_hr).show(figsize=(18,15)); img_hr.shape

print(type(img_hr_tensor))

save_image(img_hr, '/content/drive/MyDrive/Deep_Learning/my_data/testing.png')

img_hr_tensor = PIL.Image.open(r'/content/drive/MyDrive/Deep_Learning/my_data/testing.png')

img_hr_tensor.show()

p,img_hr_tensor_new,b = learn.predict(img_hr_tensor)

Image(img_hr_tensor_new).show(figsize=(18,15)); img_hr.shape